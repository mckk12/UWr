{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243dfed8",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "Deadline: lab session in the week of **25-28.11.2024**\n",
    "Each task is worth 1 point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3506f4e4",
   "metadata": {},
   "source": [
    "## 1. Maximization of Functions using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923beff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task**:\n",
    "- (a) Maximize the function $f(x) = -x^4 + 4x^2 - 2x + 1$, where $f: \\mathbb{R} \\to \\mathbb{R}$, using gradient ascent. Implement the optimization using PyTorch and plot the convergence over iterations.\n",
    "- (b) Maximize the function $f(x, y, z) = -x^2 - y^2 - z^2 + 2xy - yz + 3z$, where $f: \\mathbb{R}^3 \\to \\mathbb{R}$, using gradient ascent. Implement the optimization using PyTorch and visualize the optimization path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3f0dbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "[13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ciepi\\AppData\\Local\\Temp\\ipykernel_21384\\570214617.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  grad = torch.optim.SGD([torch.tensor(f(x))], lr=0.01, maximize=True)\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return -1*x**4 + 4*x**2 - 2*x + 13\n",
    "\n",
    "def f2(x, y, z):\n",
    "    return -1*x**2 - y**2 - z**2 + 2*x*y - y*z + 3*z\n",
    "\n",
    "x = torch.tensor([0.0], requires_grad=True)\n",
    "grad = torch.optim.SGD([torch.tensor(f(x))], lr=0.01, maximize=True)\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "for i in range(1000):\n",
    "    grad.step()\n",
    "    xs.append(i)\n",
    "    ys.append(grad.param_groups[0]['params'][0].item())\n",
    "    grad.zero_grad()\n",
    "print(grad.param_groups[0]['params'][0].item())\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f(x):\n",
    "    return -x**4 + 4*x**2 - 2*x + 1\n",
    "\n",
    "# Define the gradient of the function\n",
    "def grad_f(x):\n",
    "    return torch.autograd.grad(f(x), x)[0]\n",
    "\n",
    "# Gradient ascent parameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 100\n",
    "x = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "# Store the values of x and f(x) for plotting\n",
    "x_values = []\n",
    "f_values = []\n",
    "\n",
    "# Gradient ascent loop\n",
    "for i in range(num_iterations):\n",
    "    x_values.append(x.item())\n",
    "    f_values.append(f(x).item())\n",
    "    \n",
    "    # Compute the gradient\n",
    "    grad = grad_f(x)\n",
    "    \n",
    "    # Update x using gradient ascent\n",
    "    x = x + learning_rate * grad\n",
    "\n",
    "# Plot the convergence\n",
    "plt.plot(x_values, f_values)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Convergence of Gradient Ascent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a2ef9",
   "metadata": {},
   "source": [
    "## 2. Linear Regression in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e176bfb6",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset**: Use the following code to generate a synthetic dataset with 100 samples, each with one feature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = np.random.uniform(-10, 10, 100)\n",
    "epsilon = np.random.normal(0, 0.1, 100)\n",
    "y = 3 * X + 4 + epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902863d0",
   "metadata": {},
   "source": [
    "\n",
    "**Task**: Implement a linear regression model using PyTorch to predict the target variable $y$. Train the model to minimize the Mean Squared Error (MSE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb86152",
   "metadata": {},
   "source": [
    "## 3. Learning Rate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e12eb",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset**: Use the same dataset from Problem 2.\n",
    "\n",
    "**Task**: Modify the `gradient_descent()` function to include different learning rates (0.01, 0.1, 1.0). Visualize the convergence behaviors of gradient descent with each learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576b927",
   "metadata": {},
   "source": [
    "## 4. Polynomial Regression Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3d4f5",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset**: Use the following code to create a dataset of 100 samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0269514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "X = np.random.uniform(-5, 5, 100)\n",
    "epsilon = np.random.normal(0, 0.5, 100)\n",
    "y = 3 * X**3 - 2 * X**2 + 5 + epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd63e11",
   "metadata": {},
   "source": [
    "\n",
    "**Task**: Implement polynomial regression of degree 3 using PyTorch. Train the model and compare the training loss to that of a simple linear regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d921634c",
   "metadata": {},
   "source": [
    "## 5. Overfitting and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c7e9ce",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset**: Use the following code to generate a dataset with 150 samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc71c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(2)\n",
    "X = np.random.uniform(-10, 10, 150)\n",
    "epsilon = np.random.normal(0, 1, 150)\n",
    "y = 2 * X**2 + 3 * X + 1 + epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76778ca0",
   "metadata": {},
   "source": [
    "\n",
    "**Task**: Fit two models: (i) a linear regression model, and (ii) a polynomial regression model of degree 10. Compare the training and validation performance of both models. Apply L2 regularization to the polynomial model and observe the effect on overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9402be5",
   "metadata": {},
   "source": [
    "## 6. Custom Gradient Descent in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d461107c",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset**: Use the same dataset from Problem 2.\n",
    "\n",
    "**Task**: Implement a custom gradient descent algorithm without using an optimizer from `torch.optim`. Train a linear regression model using this custom implementation and compare the training results with those obtained using PyTorch's `SGD` optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ca8a2",
   "metadata": {},
   "source": [
    "## 7. Dataset Size and Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9398ff3",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset**: Use the following code to create three datasets of sizes 50, 100, and 500 samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(3)\n",
    "sizes = [50, 100, 500]\n",
    "datasets = []\n",
    "for size in sizes:\n",
    "    X = np.random.uniform(-10, 10, size)\n",
    "    epsilon = np.random.normal(0, 0.2, size)\n",
    "    y = 4 * X - 3 + epsilon\n",
    "    datasets.append((X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ede6b",
   "metadata": {},
   "source": [
    "\n",
    "**Task**: Train linear regression models on each dataset using PyTorch. Compare the convergence rates of gradient descent for each dataset size by plotting the training loss over epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dfa88f",
   "metadata": {},
   "source": [
    "## 8. Effect of Model Complexity on Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d4ddf",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset**: Use the following code to generate a dataset of 200 samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fa04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(4)\n",
    "X = np.random.uniform(-5, 5, 200)\n",
    "epsilon = np.random.normal(0, 0.5, 200)\n",
    "y = 2 * X**2 + X + epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6201f9",
   "metadata": {},
   "source": [
    "\n",
    "**Task**: Fit polynomial regression models of degrees 5 and 15 to the dataset. Plot the training and validation errors for both models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355a493",
   "metadata": {},
   "source": [
    "## 9. Gradient Descent for Non-linear Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8de69f",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset**: Use the following code to generate a dataset of 100 samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68e0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(5)\n",
    "X = np.random.uniform(-2 * np.pi, 2 * np.pi, 100)\n",
    "epsilon = np.random.normal(0, 0.1, 100)\n",
    "y = np.sin(X) + epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268103e",
   "metadata": {},
   "source": [
    "\n",
    "**Task**: Implement gradient descent to fit a linear model to this dataset. Discuss the challenges and limitations of fitting a linear model to non-linear data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
